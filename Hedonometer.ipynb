{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512054c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 16:50:56 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-01-09 16:50:56 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2023-01-09 16:50:56 INFO: Use device: cpu\n",
      "2023-01-09 16:50:56 INFO: Loading: tokenize\n",
      "2023-01-09 16:50:56 INFO: Loading: pos\n",
      "2023-01-09 16:50:56 INFO: Loading: lemma\n",
      "2023-01-09 16:50:56 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.signal import decimate\n",
    "from tdmh import *\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "ana = SentimentIntensityAnalyzer()\n",
    "import sklearn\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from scipy.fftpack import dct, idct\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import stanza\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#import stanza\n",
    "#stanza.download('en') # download English model\n",
    "#nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7929a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_as_string(path):\n",
    "    with open(path , encoding = 'utf-8') as text_file:\n",
    "        full_text = text_file.read()\n",
    "        full_text = re.sub( '\\s+' , ' ' , full_text)\n",
    "        return full_text\n",
    "file = 'AStrangeCrime.txt' \n",
    "full_text = get_text_as_string( file )\n",
    "#print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae9ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0eee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3309\n",
      "['BOOK I A STRANGE CRIME CHAPTER I.', 'RED LIGHT.', 'Mr. Gryce was melancholy.']\n"
     ]
    }
   ],
   "source": [
    "def get_sentences(full_text):\n",
    "    sentences = sent_tokenize(full_text.strip())\n",
    "    return sentences\n",
    "\n",
    "sentences = get_sentences(full_text)\n",
    "print( len(sentences) )\n",
    "print( sentences[0:3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94aeed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(full_text)\n",
    "# for sent in doc.sentences:\n",
    "#     new_sentence = ''\n",
    "#     for word in sent.words:\n",
    "#         new_sentence += f'{word.lemma} '  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c6802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                    | 261/3309 [00:16<03:20, 15.22it/s]"
     ]
    }
   ],
   "source": [
    "lemmatised_sentences = []\n",
    "\n",
    "\n",
    "with tqdm(total=len(sentences)) as t:\n",
    "    for s in sentences:\n",
    "\n",
    "        t.update(1)\n",
    "\n",
    "        doc = nlp(s)\n",
    "        for sent in doc.sentences:\n",
    "            new_sentence = ''\n",
    "            for word in sent.words:\n",
    "                new_sentence += f'{word.lemma} '\n",
    "            lemmatised_sentences.append(new_sentence)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "words = pd.read_csv('labMT.tsv' , sep = '\\t')\n",
    "print(words.columns)\n",
    "wordscores = dict()\n",
    "for i,row in words.iterrows():\n",
    "    wordscores[row['word']] = row['happiness_average']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhmeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b976f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment( sentences ):\n",
    "\n",
    "    all_scores = []\n",
    "    if len(sentences)>0:\n",
    "        for s in sentences:\n",
    "            words = re.split( r'\\s+' , s)\n",
    "            h = pyhmeter.HMeter( words , wordscores , deltah = 1.0 )\n",
    "            if h.happiness_score() is not None:\n",
    "                all_scores.append( h.happiness_score() )\n",
    "            \n",
    "    return all_scores\n",
    "\n",
    "scores = get_sentiment(lemmatised_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a232477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4877f",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fe906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b446e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_time = range( 0, len(scores))\n",
    "plt.figure( figsize=(12,5) )\n",
    "sns.lineplot( x = novel_time , y = scores )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc22004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(novel_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temperature['SMA_10'] = df_temperature.average_temperature.rolling(10, min_periods=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d67710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(scores):\n",
    "    all_averages = []\n",
    "    window_size = math.ceil( 0.10 * len(scores) )\n",
    "    start_index = 0\n",
    "    end_index = window_size\n",
    "    \n",
    "    #print( f'{start_index}:{end_index}' )    \n",
    "    subset = scores[start_index:end_index]\n",
    "    avg = sum(subset)/len(subset)\n",
    "    all_averages.append(avg)\n",
    "    #print(len(subset))\n",
    "    \n",
    "    while end_index < len(scores):\n",
    "        \n",
    "        start_index += 1\n",
    "        end_index = (start_index + window_size)\n",
    "        if end_index > len(scores):\n",
    "            end_index = len(scores)\n",
    "            \n",
    "        #print( f'{start_index}:{end_index}' )\n",
    "            \n",
    "        subset = scores[start_index:end_index]\n",
    "        \n",
    "        #print(len(subset))\n",
    "        avg = sum(subset)/len(subset)\n",
    "        all_averages.append(avg)\n",
    "    return all_averages\n",
    "        \n",
    "      \n",
    "ra = rolling_average(scores)\n",
    "print(len(ra))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b4c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_plot = ra\n",
    "novel_time = range( 1, len(scores_plot)+1)\n",
    "plt.figure( figsize=(10,5) )\n",
    "ax = sns.lineplot( x = novel_time , y = scores_plot )\n",
    "#ax.set_xticks(novel_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = ceil(len(ra)/100)\n",
    "ra_dec = decimate(ra,x)\n",
    "#print(ra_dec)\n",
    "new_list = []\n",
    "\n",
    "for i in range(0,len(ra)+2):\n",
    "    if i % x == 0:\n",
    "        new_list.append(ra[i])\n",
    "new_list.append(ra[-1])\n",
    "        \n",
    "print(len(new_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45506fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a9833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_plot = ra_dec\n",
    "novel_time = range( 1, len(scores_plot)+1)\n",
    "plt.figure( figsize=(10,5) )\n",
    "ax = sns.lineplot( x = novel_time , y = scores_plot )\n",
    "#ax.set_xticks(novel_time)\n",
    "img_filename = re.sub( r'txt$' , 'png' , file )\n",
    "plt.savefig(img_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a5d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
